{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MMSegmentation训练语义分割模型\n",
    "\n",
    "同济子豪兄 2023-2-13 6-12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 进入MMSegmentation主目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../mmsegmentation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/openmmlab-Camp/04-mmsegmentation-tutorial/mmsegmentation'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入工具包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mAE_h7XhPT7d",
    "outputId": "83bf0f8e-fc69-40b1-f9fe-0025724a217c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/myconda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os.path as osp\n",
    "from tqdm import tqdm\n",
    "\n",
    "import mmcv\n",
    "import mmengine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 载入config配置文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Wwnj9tRzqX_A"
   },
   "outputs": [],
   "source": [
    "from mmengine import Config\n",
    "cfg = Config.fromfile('pspnet-DubaiDataset_20230612.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QWuH14LYF2gQ"
   },
   "source": [
    "## 准备训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "jYKoSfdMF12B",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "422219ca-d7a5-4890-f09f-88c959942e64",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/13 09:22:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "------------------------------------------------------------\n",
      "System environment:\n",
      "    sys.platform: linux\n",
      "    Python: 3.8.15 (default, Nov 24 2022, 15:19:38) [GCC 11.2.0]\n",
      "    CUDA available: True\n",
      "    numpy_random_seed: 0\n",
      "    GPU 0: NVIDIA A16\n",
      "    CUDA_HOME: /usr/local/cuda\n",
      "    NVCC: Cuda compilation tools, release 11.6, V11.6.124\n",
      "    GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\n",
      "    PyTorch: 1.12.1+cu116\n",
      "    PyTorch compiling details: PyTorch built with:\n",
      "  - GCC 9.3\n",
      "  - C++ Version: 201402\n",
      "  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)\n",
      "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - NNPACK is enabled\n",
      "  - CPU capability usage: AVX2\n",
      "  - CUDA Runtime 11.6\n",
      "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86\n",
      "  - CuDNN 8.3.2  (built against CUDA 11.5)\n",
      "  - Magma 2.6.1\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.6, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
      "\n",
      "    TorchVision: 0.13.1+cu116\n",
      "    OpenCV: 4.7.0\n",
      "    MMEngine: 0.7.3\n",
      "\n",
      "Runtime environment:\n",
      "    cudnn_benchmark: True\n",
      "    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}\n",
      "    dist_cfg: {'backend': 'nccl'}\n",
      "    seed: 0\n",
      "    Distributed launcher: none\n",
      "    Distributed training: False\n",
      "    GPU number: 1\n",
      "------------------------------------------------------------\n",
      "\n",
      "06/13 09:22:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Config:\n",
      "norm_cfg = dict(type='BN', requires_grad=True)\n",
      "data_preprocessor = dict(\n",
      "    type='SegDataPreProcessor',\n",
      "    mean=[123.675, 116.28, 103.53],\n",
      "    std=[58.395, 57.12, 57.375],\n",
      "    bgr_to_rgb=True,\n",
      "    pad_val=0,\n",
      "    seg_pad_val=255,\n",
      "    size=(64, 64))\n",
      "model = dict(\n",
      "    type='EncoderDecoder',\n",
      "    data_preprocessor=dict(\n",
      "        type='SegDataPreProcessor',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        bgr_to_rgb=True,\n",
      "        pad_val=0,\n",
      "        seg_pad_val=255,\n",
      "        size=(256, 256)),\n",
      "    pretrained='open-mmlab://resnet50_v1c',\n",
      "    backbone=dict(\n",
      "        type='ResNetV1c',\n",
      "        depth=50,\n",
      "        num_stages=4,\n",
      "        out_indices=(0, 1, 2, 3),\n",
      "        dilations=(1, 1, 2, 4),\n",
      "        strides=(1, 2, 1, 1),\n",
      "        norm_cfg=dict(type='BN', requires_grad=True),\n",
      "        norm_eval=False,\n",
      "        style='pytorch',\n",
      "        contract_dilation=True),\n",
      "    decode_head=dict(\n",
      "        type='PSPHead',\n",
      "        in_channels=2048,\n",
      "        in_index=3,\n",
      "        channels=512,\n",
      "        pool_scales=(1, 2, 3, 6),\n",
      "        dropout_ratio=0.1,\n",
      "        num_classes=6,\n",
      "        norm_cfg=dict(type='BN', requires_grad=True),\n",
      "        align_corners=False,\n",
      "        loss_decode=dict(\n",
      "            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0)),\n",
      "    auxiliary_head=dict(\n",
      "        type='FCNHead',\n",
      "        in_channels=1024,\n",
      "        in_index=2,\n",
      "        channels=256,\n",
      "        num_convs=1,\n",
      "        concat_input=False,\n",
      "        dropout_ratio=0.1,\n",
      "        num_classes=6,\n",
      "        norm_cfg=dict(type='BN', requires_grad=True),\n",
      "        align_corners=False,\n",
      "        loss_decode=dict(\n",
      "            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=0.4)),\n",
      "    train_cfg=dict(),\n",
      "    test_cfg=dict(mode='whole'))\n",
      "dataset_type = 'DubaiDataset'\n",
      "data_root = 'Dubai-dataset/'\n",
      "crop_size = (256, 256)\n",
      "train_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(type='LoadAnnotations'),\n",
      "    dict(\n",
      "        type='RandomResize',\n",
      "        scale=(2048, 1024),\n",
      "        ratio_range=(0.5, 2.0),\n",
      "        keep_ratio=True),\n",
      "    dict(type='RandomCrop', crop_size=(64, 64), cat_max_ratio=0.75),\n",
      "    dict(type='RandomFlip', prob=0.5),\n",
      "    dict(type='PhotoMetricDistortion'),\n",
      "    dict(type='PackSegInputs')\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(type='Resize', scale=(2048, 1024), keep_ratio=True),\n",
      "    dict(type='LoadAnnotations'),\n",
      "    dict(type='PackSegInputs')\n",
      "]\n",
      "img_ratios = [0.5, 0.75, 1.0, 1.25, 1.5, 1.75]\n",
      "tta_pipeline = [\n",
      "    dict(type='LoadImageFromFile', file_client_args=dict(backend='disk')),\n",
      "    dict(\n",
      "        type='TestTimeAug',\n",
      "        transforms=[[{\n",
      "            'type': 'Resize',\n",
      "            'scale_factor': 0.5,\n",
      "            'keep_ratio': True\n",
      "        }, {\n",
      "            'type': 'Resize',\n",
      "            'scale_factor': 0.75,\n",
      "            'keep_ratio': True\n",
      "        }, {\n",
      "            'type': 'Resize',\n",
      "            'scale_factor': 1.0,\n",
      "            'keep_ratio': True\n",
      "        }, {\n",
      "            'type': 'Resize',\n",
      "            'scale_factor': 1.25,\n",
      "            'keep_ratio': True\n",
      "        }, {\n",
      "            'type': 'Resize',\n",
      "            'scale_factor': 1.5,\n",
      "            'keep_ratio': True\n",
      "        }, {\n",
      "            'type': 'Resize',\n",
      "            'scale_factor': 1.75,\n",
      "            'keep_ratio': True\n",
      "        }],\n",
      "                    [{\n",
      "                        'type': 'RandomFlip',\n",
      "                        'prob': 0.0,\n",
      "                        'direction': 'horizontal'\n",
      "                    }, {\n",
      "                        'type': 'RandomFlip',\n",
      "                        'prob': 1.0,\n",
      "                        'direction': 'horizontal'\n",
      "                    }], [{\n",
      "                        'type': 'LoadAnnotations'\n",
      "                    }], [{\n",
      "                        'type': 'PackSegInputs'\n",
      "                    }]])\n",
      "]\n",
      "train_dataloader = dict(\n",
      "    batch_size=8,\n",
      "    num_workers=2,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(type='InfiniteSampler', shuffle=True),\n",
      "    dataset=dict(\n",
      "        type='DubaiDataset',\n",
      "        data_root='Dubai-dataset/',\n",
      "        data_prefix=dict(\n",
      "            img_path='img_dir/train', seg_map_path='ann_dir/train'),\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(type='LoadAnnotations'),\n",
      "            dict(\n",
      "                type='RandomResize',\n",
      "                scale=(2048, 1024),\n",
      "                ratio_range=(0.5, 2.0),\n",
      "                keep_ratio=True),\n",
      "            dict(type='RandomCrop', crop_size=(64, 64), cat_max_ratio=0.75),\n",
      "            dict(type='RandomFlip', prob=0.5),\n",
      "            dict(type='PhotoMetricDistortion'),\n",
      "            dict(type='PackSegInputs')\n",
      "        ]))\n",
      "val_dataloader = dict(\n",
      "    batch_size=1,\n",
      "    num_workers=4,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(type='DefaultSampler', shuffle=False),\n",
      "    dataset=dict(\n",
      "        type='DubaiDataset',\n",
      "        data_root='Dubai-dataset/',\n",
      "        data_prefix=dict(img_path='img_dir/val', seg_map_path='ann_dir/val'),\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(type='Resize', scale=(2048, 1024), keep_ratio=True),\n",
      "            dict(type='LoadAnnotations'),\n",
      "            dict(type='PackSegInputs')\n",
      "        ]))\n",
      "test_dataloader = dict(\n",
      "    batch_size=1,\n",
      "    num_workers=4,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(type='DefaultSampler', shuffle=False),\n",
      "    dataset=dict(\n",
      "        type='DubaiDataset',\n",
      "        data_root='Dubai-dataset/',\n",
      "        data_prefix=dict(img_path='img_dir/val', seg_map_path='ann_dir/val'),\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(type='Resize', scale=(2048, 1024), keep_ratio=True),\n",
      "            dict(type='LoadAnnotations'),\n",
      "            dict(type='PackSegInputs')\n",
      "        ]))\n",
      "val_evaluator = dict(type='IoUMetric', iou_metrics=['mIoU'])\n",
      "test_evaluator = dict(type='IoUMetric', iou_metrics=['mIoU'])\n",
      "default_scope = 'mmseg'\n",
      "env_cfg = dict(\n",
      "    cudnn_benchmark=True,\n",
      "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0),\n",
      "    dist_cfg=dict(backend='nccl'))\n",
      "vis_backends = [dict(type='LocalVisBackend')]\n",
      "visualizer = dict(\n",
      "    type='SegLocalVisualizer',\n",
      "    vis_backends=[dict(type='LocalVisBackend')],\n",
      "    name='visualizer')\n",
      "log_processor = dict(by_epoch=False)\n",
      "log_level = 'INFO'\n",
      "load_from = None\n",
      "resume = False\n",
      "tta_model = dict(type='SegTTAModel')\n",
      "optimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0005)\n",
      "optim_wrapper = dict(\n",
      "    type='OptimWrapper',\n",
      "    optimizer=dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0005),\n",
      "    clip_grad=None)\n",
      "param_scheduler = [\n",
      "    dict(\n",
      "        type='PolyLR',\n",
      "        eta_min=0.0001,\n",
      "        power=0.9,\n",
      "        begin=0,\n",
      "        end=40000,\n",
      "        by_epoch=False)\n",
      "]\n",
      "train_cfg = dict(type='IterBasedTrainLoop', max_iters=3000, val_interval=400)\n",
      "val_cfg = dict(type='ValLoop')\n",
      "test_cfg = dict(type='TestLoop')\n",
      "default_hooks = dict(\n",
      "    timer=dict(type='IterTimerHook'),\n",
      "    logger=dict(type='LoggerHook', interval=100, log_metric_by_epoch=False),\n",
      "    param_scheduler=dict(type='ParamSchedulerHook'),\n",
      "    checkpoint=dict(type='CheckpointHook', by_epoch=False, interval=1500),\n",
      "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
      "    visualization=dict(type='SegVisualizationHook'))\n",
      "work_dir = './work_dirs/DubaiDataset'\n",
      "randomness = dict(seed=0)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/openmmlab-Camp/04-mmsegmentation-tutorial/mmsegmentation/mmseg/models/backbones/resnet.py:431: UserWarning: DeprecationWarning: pretrained is a deprecated, please use \"init_cfg\" instead\n",
      "  warnings.warn('DeprecationWarning: pretrained is a deprecated, '\n",
      "/mnt/openmmlab-Camp/04-mmsegmentation-tutorial/mmsegmentation/mmseg/models/builder.py:36: UserWarning: ``build_loss`` would be deprecated soon, please use ``mmseg.registry.MODELS.build()`` \n",
      "  warnings.warn('``build_loss`` would be deprecated soon, please use '\n",
      "/mnt/openmmlab-Camp/04-mmsegmentation-tutorial/mmsegmentation/mmseg/models/losses/cross_entropy_loss.py:235: UserWarning: Default ``avg_non_ignore`` is False, if you would like to ignore the certain label and average loss over non-ignore labels, which is the same with PyTorch official cross_entropy, set ``avg_non_ignore=True``.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/13 09:22:29 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
      "06/13 09:22:29 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "before_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DistSamplerSeedHook                \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) SegVisualizationHook               \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) SegVisualizationHook               \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_train:\n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_test_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) SegVisualizationHook               \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_run:\n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/openmmlab-Camp/04-mmsegmentation-tutorial/mmsegmentation/mmseg/engine/hooks/visualization_hook.py:61: UserWarning: The draw is False, it means that the hook for visualization will not take effect. The results will NOT be visualized or stored.\n",
      "  warnings.warn('The draw is False, it means that the '\n"
     ]
    }
   ],
   "source": [
    "from mmengine.runner import Runner\n",
    "from mmseg.utils import register_all_modules\n",
    "\n",
    "# register all modules in mmseg into the registries\n",
    "# do not init the default scope here because it will be init in the runner\n",
    "register_all_modules(init_default_scope=False)\n",
    "runner = Runner.from_cfg(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始训练\n",
    "\n",
    "如果遇到报错`CUDA out of memeory`，可尝试以下步骤：\n",
    "\n",
    "1. 调小 batch size\n",
    "\n",
    "2. 左上角`内核-关闭所有内核`\n",
    "\n",
    "3. 重启实例，或者使用显存更高的实例即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/13 09:22:35 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - The prefix is not set in metric class IoUMetric.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/myconda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 3, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/13 09:22:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - load model from: open-mmlab://resnet50_v1c\n",
      "06/13 09:22:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Loads checkpoint by openmmlab backend from path: open-mmlab://resnet50_v1c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.openmmlab.com/pretrain/third_party/resnet50_v1c-2cccc1ad.pth\" to /root/.cache/torch/hub/checkpoints/resnet50_v1c-2cccc1ad.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/13 09:22:48 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - The model and loaded state dict do not match exactly\n",
      "\n",
      "unexpected key in source state_dict: fc.weight, fc.bias\n",
      "\n",
      "06/13 09:22:48 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
      "06/13 09:22:48 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"HardDiskBackend\" is the alias of \"LocalBackend\" and the former will be deprecated in future.\n",
      "06/13 09:22:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Checkpoints will be saved to /mnt/openmmlab-Camp/04-mmsegmentation-tutorial/mmsegmentation/work_dirs/DubaiDataset.\n",
      "06/13 09:23:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: pspnet-DubaiDataset_20230612_20230613_092225\n",
      "06/13 09:23:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [   8/3000]  lr: 9.9984e-03  eta: 1:19:39  time: 1.5975  data_time: 0.0124  memory: 6130  loss: 0.1481  decode.loss_ce: 0.1064  decode.acc_seg: 44.2963  aux.loss_ce: 0.0417  aux.acc_seg: 43.8477\n",
      "06/13 09:23:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 100/3000]  lr: 9.9779e-03  eta: 0:34:39  time: 0.6403  data_time: 0.0083  memory: 3614  loss: 0.1069  decode.loss_ce: 0.0757  decode.acc_seg: 72.5494  aux.loss_ce: 0.0312  aux.acc_seg: 75.5219\n",
      "06/13 09:25:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 200/3000]  lr: 9.9557e-03  eta: 0:31:42  time: 0.6415  data_time: 0.0081  memory: 3614  loss: 0.0991  decode.loss_ce: 0.0709  decode.acc_seg: 61.6180  aux.loss_ce: 0.0281  aux.acc_seg: 55.0110\n",
      "06/13 09:27:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 400/3000]  lr: 9.9111e-03  eta: 0:28:38  time: 0.6432  data_time: 0.0083  memory: 3614  loss: 0.0796  decode.loss_ce: 0.0563  decode.acc_seg: 71.1639  aux.loss_ce: 0.0233  aux.acc_seg: 71.9055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/myconda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 3, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/13 09:27:43 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - per class results:\n",
      "06/13 09:27:43 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "+------------+-------+-------+\n",
      "|   Class    |  IoU  |  Acc  |\n",
      "+------------+-------+-------+\n",
      "|    Land    | 22.37 | 24.11 |\n",
      "|    Road    | 14.87 | 88.63 |\n",
      "|  Building  |  4.83 |  4.91 |\n",
      "| Vegetation | 14.52 | 14.91 |\n",
      "|   Water    |  36.5 | 51.66 |\n",
      "| Unlabeled  |  0.0  |  0.0  |\n",
      "+------------+-------+-------+\n",
      "06/13 09:27:43 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(val) [15/15]    aAcc: 31.0700  mIoU: 15.5100  mAcc: 30.7000  data_time: 0.1159  time: 2.0327\n",
      "06/13 09:28:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 500/3000]  lr: 9.8888e-03  eta: 0:27:23  time: 0.6425  data_time: 0.0077  memory: 5769  loss: 0.0843  decode.loss_ce: 0.0598  decode.acc_seg: 66.5741  aux.loss_ce: 0.0244  aux.acc_seg: 69.0643\n",
      "06/13 09:29:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 600/3000]  lr: 9.8665e-03  eta: 0:26:11  time: 0.6426  data_time: 0.0082  memory: 3614  loss: 0.0789  decode.loss_ce: 0.0560  decode.acc_seg: 47.6868  aux.loss_ce: 0.0228  aux.acc_seg: 50.9094\n",
      "06/13 09:30:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 700/3000]  lr: 9.8442e-03  eta: 0:25:02  time: 0.6433  data_time: 0.0086  memory: 3614  loss: 0.0849  decode.loss_ce: 0.0606  decode.acc_seg: 62.6343  aux.loss_ce: 0.0242  aux.acc_seg: 63.9893\n",
      "06/13 09:32:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 800/3000]  lr: 9.8218e-03  eta: 0:23:54  time: 0.6424  data_time: 0.0077  memory: 3614  loss: 0.0746  decode.loss_ce: 0.0532  decode.acc_seg: 53.4485  aux.loss_ce: 0.0215  aux.acc_seg: 52.5208\n",
      "06/13 09:32:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - per class results:\n",
      "06/13 09:32:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "+------------+-------+-------+\n",
      "|   Class    |  IoU  |  Acc  |\n",
      "+------------+-------+-------+\n",
      "|    Land    | 61.88 | 91.08 |\n",
      "|    Road    |  29.4 | 36.21 |\n",
      "|  Building  | 19.97 | 21.51 |\n",
      "| Vegetation | 35.78 | 47.43 |\n",
      "|   Water    | 49.71 | 55.35 |\n",
      "| Unlabeled  |  0.0  |  0.0  |\n",
      "+------------+-------+-------+\n",
      "06/13 09:32:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(val) [15/15]    aAcc: 66.5200  mIoU: 32.7900  mAcc: 41.9300  data_time: 0.0052  time: 0.4113\n",
      "06/13 09:33:10 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 900/3000]  lr: 9.7995e-03  eta: 0:22:47  time: 0.6426  data_time: 0.0078  memory: 3614  loss: 0.0731  decode.loss_ce: 0.0522  decode.acc_seg: 73.3368  aux.loss_ce: 0.0209  aux.acc_seg: 68.7073\n",
      "06/13 09:34:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: pspnet-DubaiDataset_20230612_20230613_092225\n",
      "06/13 09:34:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [1000/3000]  lr: 9.7772e-03  eta: 0:21:40  time: 0.6441  data_time: 0.0082  memory: 3614  loss: 0.0728  decode.loss_ce: 0.0517  decode.acc_seg: 73.4894  aux.loss_ce: 0.0211  aux.acc_seg: 70.9412\n",
      "06/13 09:35:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [1100/3000]  lr: 9.7549e-03  eta: 0:20:34  time: 0.6438  data_time: 0.0080  memory: 3614  loss: 0.0764  decode.loss_ce: 0.0545  decode.acc_seg: 68.9606  aux.loss_ce: 0.0219  aux.acc_seg: 69.2688\n",
      "06/13 09:36:23 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [1200/3000]  lr: 9.7325e-03  eta: 0:19:28  time: 0.6440  data_time: 0.0082  memory: 3614  loss: 0.0650  decode.loss_ce: 0.0457  decode.acc_seg: 70.3857  aux.loss_ce: 0.0192  aux.acc_seg: 67.8192\n",
      "06/13 09:36:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - per class results:\n",
      "06/13 09:36:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "+------------+-------+-------+\n",
      "|   Class    |  IoU  |  Acc  |\n",
      "+------------+-------+-------+\n",
      "|    Land    |  58.6 | 95.78 |\n",
      "|    Road    | 23.95 | 27.16 |\n",
      "|  Building  | 10.87 | 11.17 |\n",
      "| Vegetation | 10.85 | 11.07 |\n",
      "|   Water    |  43.8 | 48.76 |\n",
      "| Unlabeled  |  0.0  |  0.0  |\n",
      "+------------+-------+-------+\n",
      "06/13 09:36:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(val) [15/15]    aAcc: 62.4700  mIoU: 24.6800  mAcc: 32.3200  data_time: 0.0061  time: 0.4145\n",
      "06/13 09:37:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [1300/3000]  lr: 9.7102e-03  eta: 0:18:22  time: 0.6453  data_time: 0.0083  memory: 3614  loss: 0.0672  decode.loss_ce: 0.0470  decode.acc_seg: 63.0829  aux.loss_ce: 0.0201  aux.acc_seg: 60.9344\n",
      "06/13 09:38:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [1400/3000]  lr: 9.6878e-03  eta: 0:17:17  time: 0.6426  data_time: 0.0073  memory: 3614  loss: 0.0658  decode.loss_ce: 0.0466  decode.acc_seg: 77.6154  aux.loss_ce: 0.0191  aux.acc_seg: 77.3956\n",
      "06/13 09:39:43 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [1500/3000]  lr: 9.6655e-03  eta: 0:16:12  time: 0.6437  data_time: 0.0081  memory: 3614  loss: 0.0798  decode.loss_ce: 0.0559  decode.acc_seg: 64.8438  aux.loss_ce: 0.0239  aux.acc_seg: 64.0656\n",
      "06/13 09:39:43 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 1500 iterations\n",
      "06/13 09:40:54 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [1600/3000]  lr: 9.6431e-03  eta: 0:15:12  time: 0.6436  data_time: 0.0081  memory: 3614  loss: 0.0749  decode.loss_ce: 0.0536  decode.acc_seg: 58.3801  aux.loss_ce: 0.0214  aux.acc_seg: 59.1827\n",
      "06/13 09:41:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - per class results:\n",
      "06/13 09:41:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "+------------+-------+-------+\n",
      "|   Class    |  IoU  |  Acc  |\n",
      "+------------+-------+-------+\n",
      "|    Land    | 57.61 | 98.53 |\n",
      "|    Road    |  9.98 | 10.44 |\n",
      "|  Building  |  7.68 |  7.82 |\n",
      "| Vegetation |  5.72 |  5.75 |\n",
      "|   Water    | 36.89 | 39.86 |\n",
      "| Unlabeled  |  0.0  |  0.0  |\n",
      "+------------+-------+-------+\n",
      "06/13 09:41:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(val) [15/15]    aAcc: 60.0600  mIoU: 19.6500  mAcc: 27.0600  data_time: 0.0057  time: 0.4129\n",
      "06/13 09:42:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [1700/3000]  lr: 9.6207e-03  eta: 0:14:06  time: 0.6428  data_time: 0.0078  memory: 3614  loss: 0.0645  decode.loss_ce: 0.0455  decode.acc_seg: 65.4510  aux.loss_ce: 0.0190  aux.acc_seg: 69.0552\n",
      "06/13 09:43:09 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [1800/3000]  lr: 9.5983e-03  eta: 0:13:01  time: 0.6416  data_time: 0.0077  memory: 3614  loss: 0.0666  decode.loss_ce: 0.0467  decode.acc_seg: 66.9952  aux.loss_ce: 0.0199  aux.acc_seg: 65.5701\n",
      "06/13 09:44:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [1900/3000]  lr: 9.5760e-03  eta: 0:11:55  time: 0.6429  data_time: 0.0087  memory: 3614  loss: 0.0725  decode.loss_ce: 0.0514  decode.acc_seg: 70.4559  aux.loss_ce: 0.0211  aux.acc_seg: 66.9983\n",
      "06/13 09:45:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: pspnet-DubaiDataset_20230612_20230613_092225\n",
      "06/13 09:45:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [2000/3000]  lr: 9.5536e-03  eta: 0:10:50  time: 0.6423  data_time: 0.0084  memory: 3614  loss: 0.0672  decode.loss_ce: 0.0472  decode.acc_seg: 74.7772  aux.loss_ce: 0.0200  aux.acc_seg: 74.5575\n",
      "06/13 09:45:24 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - per class results:\n",
      "06/13 09:45:24 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "+------------+-------+-------+\n",
      "|   Class    |  IoU  |  Acc  |\n",
      "+------------+-------+-------+\n",
      "|    Land    | 59.72 |  94.5 |\n",
      "|    Road    | 35.83 | 43.16 |\n",
      "|  Building  |  7.89 |  8.01 |\n",
      "| Vegetation |  31.3 | 35.27 |\n",
      "|   Water    | 37.63 | 38.66 |\n",
      "| Unlabeled  |  0.0  |  0.0  |\n",
      "+------------+-------+-------+\n",
      "06/13 09:45:24 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(val) [15/15]    aAcc: 64.4800  mIoU: 28.7300  mAcc: 36.6000  data_time: 0.0048  time: 0.4110\n",
      "06/13 09:46:28 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [2100/3000]  lr: 9.5312e-03  eta: 0:09:44  time: 0.6418  data_time: 0.0079  memory: 3614  loss: 0.0691  decode.loss_ce: 0.0485  decode.acc_seg: 57.4127  aux.loss_ce: 0.0206  aux.acc_seg: 56.8298\n",
      "06/13 09:47:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [2200/3000]  lr: 9.5088e-03  eta: 0:08:39  time: 0.6432  data_time: 0.0085  memory: 3614  loss: 0.0624  decode.loss_ce: 0.0437  decode.acc_seg: 67.1478  aux.loss_ce: 0.0187  aux.acc_seg: 62.4176\n",
      "06/13 09:48:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [2300/3000]  lr: 9.4864e-03  eta: 0:07:34  time: 0.6428  data_time: 0.0083  memory: 3614  loss: 0.0705  decode.loss_ce: 0.0498  decode.acc_seg: 61.5326  aux.loss_ce: 0.0206  aux.acc_seg: 61.8805\n",
      "06/13 09:49:41 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [2400/3000]  lr: 9.4640e-03  eta: 0:06:29  time: 0.6428  data_time: 0.0081  memory: 3614  loss: 0.0590  decode.loss_ce: 0.0413  decode.acc_seg: 80.0659  aux.loss_ce: 0.0177  aux.acc_seg: 79.7607\n",
      "06/13 09:49:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - per class results:\n",
      "06/13 09:49:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "+------------+-------+-------+\n",
      "|   Class    |  IoU  |  Acc  |\n",
      "+------------+-------+-------+\n",
      "|    Land    | 63.88 | 92.72 |\n",
      "|    Road    | 41.43 | 51.36 |\n",
      "|  Building  | 29.21 | 31.82 |\n",
      "| Vegetation | 37.84 | 41.16 |\n",
      "|   Water    |  55.8 | 57.84 |\n",
      "| Unlabeled  |  0.0  |  0.0  |\n",
      "+------------+-------+-------+\n",
      "06/13 09:49:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(val) [15/15]    aAcc: 70.3500  mIoU: 38.0300  mAcc: 45.8100  data_time: 0.0056  time: 0.4121\n",
      "06/13 09:50:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [2500/3000]  lr: 9.4416e-03  eta: 0:05:24  time: 0.6429  data_time: 0.0083  memory: 3614  loss: 0.0642  decode.loss_ce: 0.0456  decode.acc_seg: 67.6910  aux.loss_ce: 0.0185  aux.acc_seg: 70.8801\n",
      "06/13 09:51:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [2600/3000]  lr: 9.4191e-03  eta: 0:04:19  time: 0.6434  data_time: 0.0084  memory: 3614  loss: 0.0661  decode.loss_ce: 0.0463  decode.acc_seg: 72.6898  aux.loss_ce: 0.0197  aux.acc_seg: 74.3866\n",
      "06/13 09:53:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [2700/3000]  lr: 9.3967e-03  eta: 0:03:14  time: 0.6429  data_time: 0.0085  memory: 3614  loss: 0.0720  decode.loss_ce: 0.0508  decode.acc_seg: 59.8267  aux.loss_ce: 0.0212  aux.acc_seg: 61.5509\n",
      "06/13 09:54:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [2800/3000]  lr: 9.3743e-03  eta: 0:02:09  time: 0.6419  data_time: 0.0078  memory: 3614  loss: 0.0622  decode.loss_ce: 0.0432  decode.acc_seg: 69.5557  aux.loss_ce: 0.0190  aux.acc_seg: 67.1967\n",
      "06/13 09:54:10 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - per class results:\n",
      "06/13 09:54:10 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "+------------+-------+-------+\n",
      "|   Class    |  IoU  |  Acc  |\n",
      "+------------+-------+-------+\n",
      "|    Land    | 64.96 | 92.28 |\n",
      "|    Road    | 30.08 | 32.78 |\n",
      "|  Building  |  11.6 | 11.83 |\n",
      "| Vegetation | 41.56 | 71.05 |\n",
      "|   Water    | 53.35 | 60.28 |\n",
      "| Unlabeled  |  0.0  |  0.0  |\n",
      "+------------+-------+-------+\n",
      "06/13 09:54:10 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(val) [15/15]    aAcc: 68.0900  mIoU: 33.5900  mAcc: 44.7000  data_time: 0.0057  time: 0.4135\n",
      "06/13 09:55:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [2900/3000]  lr: 9.3518e-03  eta: 0:01:04  time: 0.6429  data_time: 0.0076  memory: 3614  loss: 0.0662  decode.loss_ce: 0.0467  decode.acc_seg: 80.6610  aux.loss_ce: 0.0195  aux.acc_seg: 78.0457\n",
      "06/13 09:56:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: pspnet-DubaiDataset_20230612_20230613_092225\n",
      "06/13 09:56:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [3000/3000]  lr: 9.3294e-03  eta: 0:00:00  time: 0.6422  data_time: 0.0082  memory: 3614  loss: 0.0698  decode.loss_ce: 0.0495  decode.acc_seg: 68.8171  aux.loss_ce: 0.0203  aux.acc_seg: 66.7603\n",
      "06/13 09:56:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 3000 iterations\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EncoderDecoder(\n",
       "  (data_preprocessor): SegDataPreProcessor()\n",
       "  (backbone): ResNetV1c(\n",
       "    (stem): Sequential(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU(inplace=True)\n",
       "    )\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): ResLayer(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): ResLayer(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): ResLayer(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): ResLayer(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  init_cfg={'type': 'Pretrained', 'checkpoint': 'open-mmlab://resnet50_v1c'}\n",
       "  (decode_head): PSPHead(\n",
       "    input_transform=None, ignore_index=255, align_corners=False\n",
       "    (loss_decode): CrossEntropyLoss(avg_non_ignore=False)\n",
       "    (conv_seg): Conv2d(512, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (dropout): Dropout2d(p=0.1, inplace=False)\n",
       "    (psp_modules): PPM(\n",
       "      (0): Sequential(\n",
       "        (0): AdaptiveAvgPool2d(output_size=1)\n",
       "        (1): ConvModule(\n",
       "          (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): AdaptiveAvgPool2d(output_size=2)\n",
       "        (1): ConvModule(\n",
       "          (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): AdaptiveAvgPool2d(output_size=3)\n",
       "        (1): ConvModule(\n",
       "          (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): AdaptiveAvgPool2d(output_size=6)\n",
       "        (1): ConvModule(\n",
       "          (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activate): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (bottleneck): ConvModule(\n",
       "      (conv): Conv2d(4096, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activate): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  init_cfg={'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}\n",
       "  (auxiliary_head): FCNHead(\n",
       "    input_transform=None, ignore_index=255, align_corners=False\n",
       "    (loss_decode): CrossEntropyLoss(avg_non_ignore=False)\n",
       "    (conv_seg): Conv2d(256, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (dropout): Dropout2d(p=0.1, inplace=False)\n",
       "    (convs): Sequential(\n",
       "      (0): ConvModule(\n",
       "        (conv): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activate): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  init_cfg={'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runner.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "MMSegmentation Tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "myconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "20d4b83e0c8b3730b580c42434163d64f4b735d580303a8fade7c849d4d29eba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
